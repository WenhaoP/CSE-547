{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wenhao\\miniconda3\\lib\\site-packages\\pyspark\\pandas\\__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries we will need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "# create the Spark Session\n",
    "spark = SparkSession.builder.appName(\"Q1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Frequent Pairs:\n",
      "\n",
      "+--------------------+---------+\n",
      "|       Frequent_pair|Frequency|\n",
      "+--------------------+---------+\n",
      "|[DAI62779, ELE17451]|     1592|\n",
      "|[FRO40251, SNA80324]|     1412|\n",
      "|[DAI75645, FRO40251]|     1254|\n",
      "|[FRO40251, GRO85051]|     1213|\n",
      "|[DAI62779, GRO73461]|     1139|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Top 5 Confident Rules:\n",
      "\n",
      "+--------+--------+------------------+\n",
      "|    Left|   Right|        Confidence|\n",
      "+--------+--------+------------------+\n",
      "|DAI93865|FRO40251|               1.0|\n",
      "|GRO85051|FRO40251| 0.999176276771005|\n",
      "|GRO38636|FRO40251|0.9906542056074766|\n",
      "|ELE12951|FRO40251|0.9905660377358491|\n",
      "|DAI88079|FRO40251|0.9867256637168141|\n",
      "+--------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = 100 # support threshold\n",
    "\n",
    "# read and preprocess the data\n",
    "browsing = spark.read.csv(\"hw1-bundle/hw1-bundle/q2/data/browsing.txt\", sep='\\t')\n",
    "browsing = browsing.toDF(\"Items\")\n",
    "browsing = browsing.withColumn(\"Items\", F.split(\"Items\", \" \").cast(\"array<string>\"))\n",
    "\n",
    "# find the frequent items\n",
    "browsing_exploded = browsing.withColumn(\"Item\", F.explode(browsing[\"Items\"]))\n",
    "browsing_exploded = browsing_exploded.filter(F.length(\"Item\") == 8)\n",
    "browsing_grouped = browsing_exploded.groupBy(\"Item\").count().withColumnRenamed(\"count\", \"Frequency\")\n",
    "freq_items = browsing_grouped.filter(F.col(\"Frequency\") >= s)\n",
    "\n",
    "# find the frequent pairs\n",
    "frequent_pairs = browsing.withColumn(\"Frequent_items\", F.lit(freq_items.select(\"Item\").rdd.flatMap(lambda x: x).collect()))\n",
    "frequent_pairs = frequent_pairs.withColumn(\"Frequent_items\", F.array_intersect(\"Items\", \"Frequent_items\"))\n",
    "frequent_pairs = frequent_pairs.withColumn(\"Frequent_pairs\", (F.udf(lambda x: list(itertools.combinations(x, 2)), \"array<array<string>>\"))(F.col(\"Frequent_items\")))\n",
    "frequent_pairs = frequent_pairs.withColumn(\"Frequent_pair\", F.explode(\"Frequent_pairs\")).select(\"Frequent_pair\")\n",
    "frequent_pairs = frequent_pairs.withColumn(\"Frequent_pair\", F.sort_array(\"Frequent_pair\"))\n",
    "frequent_pairs = frequent_pairs.groupBy(\"Frequent_pair\").count().withColumnRenamed(\"count\", \"Frequency\")\n",
    "frequent_pairs = frequent_pairs.filter(F.col(\"Frequency\") >= s).sort([F.desc(\"Frequency\"), F.asc(\"Frequent_pair\")])\n",
    "\n",
    "print(\"Top 5 Frequent Pairs:\\n\")\n",
    "frequent_pairs.show(5)\n",
    "\n",
    "# generate the association rules from the frequent pairs and compute their confidence scores\n",
    "frequent_pairs = frequent_pairs.withColumns({\n",
    "    \"X\": (F.udf(lambda x: x[0]))(F.col(\"Frequent_pair\")),\n",
    "    \"Y\": (F.udf(lambda x: x[1]))(F.col(\"Frequent_pair\")),\n",
    "})\n",
    "frequent_pairs = frequent_pairs.join(freq_items.withColumnRenamed(\"Frequency\", \"X_frequency\"), frequent_pairs[\"X\"] == freq_items[\"Item\"], \"left\").drop(\"Item\")\n",
    "frequent_pairs = frequent_pairs.join(freq_items.withColumnRenamed(\"Frequency\", \"Y_frequency\"), frequent_pairs[\"Y\"] == freq_items[\"Item\"], \"left\").drop(\"Item\")\n",
    "frequent_pairs = frequent_pairs.withColumns({\n",
    "    \"Forward_conf\": F.col(\"Frequency\") / F.col(\"X_frequency\"),\n",
    "    \"Backward_conf\": F.col(\"Frequency\") / F.col(\"Y_frequency\")\n",
    "})\n",
    "forward_rules = frequent_pairs.select([\"X\", \"Y\", \"Forward_conf\"]).withColumnsRenamed({\n",
    "    \"X\": \"Left\",\n",
    "    \"Y\": \"Right\",\n",
    "    \"Forward_conf\": \"Confidence\"\n",
    "})\n",
    "backward_rules = frequent_pairs.select([\"Y\", \"X\", \"Backward_conf\"]).withColumnsRenamed({\n",
    "    \"Y\": \"Left\",\n",
    "    \"X\": \"Right\", \n",
    "    \"Backward_conf\": \"Confidence\"})\n",
    "frequent_pairs = frequent_pairs.select([\"Frequent_pair\", \"Frequency\"])\n",
    "pair_rules = forward_rules.union(backward_rules)\n",
    "pair_rules = pair_rules.sort([F.desc(\"Confidence\"), F.asc(\"Left\")])\n",
    "\n",
    "print(\"Top 5 Confident Rules:\\n\")\n",
    "pair_rules.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Frequent Pairs:\n",
      "\n",
      "+------------------------------+---------+\n",
      "|Frequent_triple               |Frequency|\n",
      "+------------------------------+---------+\n",
      "|[DAI75645, FRO40251, SNA80324]|550      |\n",
      "|[DAI62779, FRO40251, SNA80324]|476      |\n",
      "|[FRO40251, GRO85051, SNA80324]|471      |\n",
      "|[DAI62779, ELE92920, SNA18336]|432      |\n",
      "|[DAI62779, DAI75645, SNA80324]|421      |\n",
      "+------------------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Top 5 Confident Rules:\n",
      "\n",
      "+--------------------+--------+----------+\n",
      "|Left                |Right   |Confidence|\n",
      "+--------------------+--------+----------+\n",
      "|[DAI23334, ELE92920]|DAI62779|1.0       |\n",
      "|[DAI31081, GRO85051]|FRO40251|1.0       |\n",
      "|[DAI55911, GRO85051]|FRO40251|1.0       |\n",
      "|[DAI62779, DAI88079]|FRO40251|1.0       |\n",
      "|[DAI75645, GRO85051]|FRO40251|1.0       |\n",
      "+--------------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the frequent triples\n",
    "frequent_triples = browsing.withColumn(\"Frequent_items\", F.lit(freq_items.select(\"Item\").rdd.flatMap(lambda x: x).collect()))\n",
    "frequent_triples = frequent_triples.withColumn(\"Frequent_items\", F.array_intersect(\"Items\", \"Frequent_items\"))\n",
    "frequent_triples = frequent_triples.withColumn(\"Frequent_triples\", (F.udf(lambda x: list(itertools.combinations(x, 3)), \"array<array<string>>\"))(F.col(\"Frequent_items\")))\n",
    "frequent_triples = frequent_triples.withColumn(\"Frequent_triple\", F.explode(\"Frequent_triples\")).select(\"Frequent_triple\")\n",
    "\n",
    "frequent_triples = frequent_triples.withColumn(\"Frequent_triple\", F.sort_array(\"Frequent_triple\"))\n",
    "frequent_triples = frequent_triples.groupBy(\"Frequent_triple\").count().withColumnRenamed(\"count\", \"Frequency\")\n",
    "frequent_triples = frequent_triples.filter(F.col(\"Frequency\") >= s).sort([F.desc(\"Frequency\"), F.asc(\"Frequent_triple\")])\n",
    "\n",
    "print(\"Top 5 Frequent Triples:\\n\")\n",
    "frequent_triples.show(5, False)\n",
    "\n",
    "# generate the association rules from the frequent triples and compute their confidence scores\n",
    "frequent_triples = frequent_triples.withColumns({\n",
    "    \"X\": (F.udf(lambda x: x[0], \"string\"))(F.col(\"Frequent_triple\")),\n",
    "    \"Y\": (F.udf(lambda x: x[1], \"string\"))(F.col(\"Frequent_triple\")),\n",
    "    \"Z\": (F.udf(lambda x: x[2], \"string\"))(F.col(\"Frequent_triple\")),\n",
    "    \"X_Y\": (F.udf(lambda x: [x[0]] + [x[1]], \"array<string>\"))(F.col(\"Frequent_triple\")),\n",
    "    \"X_Z\": (F.udf(lambda x: [x[0]] + [x[2]], \"array<string>\"))(F.col(\"Frequent_triple\")),\n",
    "    \"Y_Z\": (F.udf(lambda x: [x[1]] + [x[2]], \"array<string>\"))(F.col(\"Frequent_triple\")),\n",
    "})\n",
    "\n",
    "frequent_triples = frequent_triples.join(frequent_pairs.withColumnRenamed(\"Frequency\", \"X_Y_frequency\"), frequent_triples[\"X_Y\"] == frequent_pairs[\"Frequent_pair\"], \"left\").drop(\"Frequent_pair\")\n",
    "frequent_triples = frequent_triples.join(frequent_pairs.withColumnRenamed(\"Frequency\", \"X_Z_frequency\"), frequent_triples[\"X_Z\"] == frequent_pairs[\"Frequent_pair\"], \"left\").drop(\"Frequent_pair\")\n",
    "frequent_triples = frequent_triples.join(frequent_pairs.withColumnRenamed(\"Frequency\", \"Y_Z_frequency\"), frequent_triples[\"Y_Z\"] == frequent_pairs[\"Frequent_pair\"], \"left\").drop(\"Frequent_pair\")\n",
    "\n",
    "frequent_triples = frequent_triples.withColumns({\n",
    "    \"X_Y_to_Z_conf\": F.col(\"Frequency\") / F.col(\"X_Y_frequency\"),\n",
    "    \"X_Z_to_Y_conf\": F.col(\"Frequency\") / F.col(\"X_Z_frequency\"),\n",
    "    \"Y_Z_to_X_conf\": F.col(\"Frequency\") / F.col(\"Y_Z_frequency\"),\n",
    "})\n",
    "\n",
    "X_Y_to_Z_rules = frequent_triples.select([\"X_Y\", \"Z\", \"X_Y_to_Z_conf\"]).withColumnsRenamed({\n",
    "    \"X_Y\": \"Left\",\n",
    "    \"Z\": \"Right\",\n",
    "    \"X_Y_to_Z_conf\": \"Confidence\"\n",
    "})\n",
    "X_Z_to_Y_rules = frequent_triples.select([\"X_Z\", \"Y\", \"X_Z_to_Y_conf\"]).withColumnsRenamed({\n",
    "    \"X_Z\": \"Left\",\n",
    "    \"Y\": \"Right\",\n",
    "    \"X_Z_to_Y_conf\": \"Confidence\"\n",
    "})\n",
    "Y_Z_to_X_rules = frequent_triples.select([\"Y_Z\", \"X\", \"Y_Z_to_X_conf\"]).withColumnsRenamed({\n",
    "    \"Y_Z\": \"Left\",\n",
    "    \"X\": \"Right\",\n",
    "    \"Y_Z_to_X_conf\": \"Confidence\"\n",
    "})\n",
    "triple_rules = X_Y_to_Z_rules.union(X_Z_to_Y_rules).union(Y_Z_to_X_rules)\n",
    "triple_rules = triple_rules.sort([F.desc(\"Confidence\"), F.asc(\"Left\")])\n",
    "print(\"Top 5 Confident Rules:\\n\")\n",
    "triple_rules.show(5, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
